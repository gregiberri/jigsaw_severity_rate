id: "sklearn"
env:
  result_dir: 'results'
  random_seed: 0
  epochs: 10
  solver: 'sklearn-solver'
data:
  name: "jigsaw_dataset"
  params:
    dataset_path: '/home/albert/data/jigsaw'
    test_filename: 'comments_to_score.csv'

    category_weights:
      'obscene': 0.16
      'toxic': 0.32
      'threat': 1.5
      'insult': 0.64
      'severe_toxic': 1.5
      'identity_hate': 1.5

    gpu_to_use: 0
    workers: 8
    load_into_memory: false

    clean_text: True

    tokenizer:
      name: "WordPiece"
      pretrained: False
      params:
        unk_token: "[UNK]"
      tokenize_params:
        truncation: True
        add_special_tokens: True
        max_length: 128
        padding: 'max_length'

      normalizer:
        name: "BertNormalizer"
        params:
          lowercase: True

      pre_tokenizer:
        name: "BertPreTokenizer"
        params: {}

      trainer:
        name: "WordPieceTrainer"
        params:
          vocab_size: 15000
          special_tokens: ["[UNK]", "[PAD]", "[CLS]", "[SEP]", "[MASK]"]

    vectorizer:
      name: 'TfidfVectorizer'

model:
  name: "Ridge"
  params:
    alpha: 0.8