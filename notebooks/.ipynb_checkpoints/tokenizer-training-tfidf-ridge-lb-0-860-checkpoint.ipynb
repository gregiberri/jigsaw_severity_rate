{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>How to train a Huggingface Tokenizer + TFIDF + RIDGE</center></h1>     \n",
    "\n",
    "<center><img src = \"https://i.imgur.com/iRX7hwu.png\" width = \"1000\" height = \"400\"/></center>           \n",
    "\n",
    "This notebook was inspided on the following other two notebooks:\n",
    "* https://www.kaggle.com/vitaleey/tfidf-ridge\n",
    "* https://www.kaggle.com/pablorosa01/naive-bayes-modeling-base-line\n",
    "\n",
    "<h3 style='background:orange; color:black'><center>Consider upvoting this notebook if you found it helpful.</center></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA_PATH = \"/kaggle/input/jigsaw-toxic-comment-classification-challenge/train.csv\"\n",
    "VALID_DATA_PATH = \"/kaggle/input/jigsaw-toxic-severity-rating/validation_data.csv\"\n",
    "TEST_DATA_PATH = \"/kaggle/input/jigsaw-toxic-severity-rating/comments_to_score.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(TRAIN_DATA_PATH)\n",
    "df_valid = pd.read_csv(VALID_DATA_PATH)\n",
    "df_test = pd.read_csv(TEST_DATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scoring training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_mtpl = {'obscene': 0.16, 'toxic': 0.32, 'threat': 1.5, \n",
    "            'insult': 0.64, 'severe_toxic': 1.5, 'identity_hate': 1.5}\n",
    "\n",
    "for category in cat_mtpl:\n",
    "    df_train[category] = df_train[category] * cat_mtpl[category]\n",
    "\n",
    "df_train['score'] = df_train.loc[:, 'toxic':'identity_hate'].mean(axis=1)\n",
    "\n",
    "df_train['y'] = df_train['score']\n",
    "\n",
    "min_len = (df_train['y'] > 0).sum()  # len of toxic comments\n",
    "df_y0_undersample = df_train[df_train['y'] == 0].sample(n=min_len, random_state=41)  # take non toxic comments\n",
    "df_train_new = pd.concat([df_train[df_train['y'] > 0], df_y0_undersample])  # make new df\n",
    "df_train_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import (\n",
    "    decoders,\n",
    "    models,\n",
    "    normalizers,\n",
    "    pre_tokenizers,\n",
    "    processors,\n",
    "    trainers,\n",
    "    Tokenizer,\n",
    ")\n",
    "\n",
    "raw_tokenizer = Tokenizer(models.WordPiece(unk_token=\"[UNK]\"))\n",
    "raw_tokenizer.normalizer = normalizers.BertNormalizer(lowercase=True)\n",
    "raw_tokenizer.pre_tokenizer = pre_tokenizers.BertPreTokenizer()\n",
    "special_tokens = [\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
    "trainer = trainers.WordPieceTrainer(vocab_size=25000, special_tokens=special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "dataset = Dataset.from_pandas(df_train_new[['comment_text']])\n",
    "\n",
    "def get_training_corpus():\n",
    "    for i in range(0, len(dataset), 1000):\n",
    "        yield dataset[i : i + 1000][\"comment_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_object=raw_tokenizer,\n",
    "    unk_token=\"[UNK]\",\n",
    "    pad_token=\"[PAD]\",\n",
    "    cls_token=\"[CLS]\",\n",
    "    sep_token=\"[SEP]\",\n",
    "    mask_token=\"[MASK]\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy_fun(doc):\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df_train_new['y']\n",
    "comments = df_train_new['comment_text']\n",
    "tokenized_comments = tokenizer(comments.to_list())['input_ids']\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    analyzer = 'word',\n",
    "    tokenizer = dummy_fun,\n",
    "    preprocessor = dummy_fun,\n",
    "    token_pattern = None)\n",
    "\n",
    "comments_tr = vectorizer.fit_transform(tokenized_comments)\n",
    "comments_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = Ridge(random_state=42, alpha=0.8)\n",
    "regressor.fit(comments_tr, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess val data\n",
    "less_toxic_comments = df_valid['less_toxic']\n",
    "more_toxic_comments = df_valid['more_toxic']\n",
    "\n",
    "less_toxic_comments = tokenizer(less_toxic_comments.to_list())['input_ids']\n",
    "more_toxic_comments = tokenizer(more_toxic_comments.to_list())['input_ids']\n",
    "\n",
    "less_toxic = vectorizer.transform(less_toxic_comments)\n",
    "more_toxic = vectorizer.transform(more_toxic_comments)\n",
    "\n",
    "# make predictions\n",
    "y_pred_less = regressor.predict(less_toxic)\n",
    "y_pred_more = regressor.predict(more_toxic)\n",
    "\n",
    "(y_pred_less < y_pred_more).mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Tokenizer (deberta-v3): 0.6699880430450379\n",
    "* Tokenizer (trained): 0.6674970107612594\n",
    "* Tokenizer (trained + dirty): 0.6716819449980072\n",
    "\n",
    "** Be careful, this results suggest that the 0.86 LB score is not reliable!!! Use at your own risk!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions and load submission.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = df_test['text']\n",
    "texts = tokenizer(texts.to_list())['input_ids']\n",
    "texts = vectorizer.transform(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['prediction'] = regressor.predict(texts)\n",
    "df_test = df_test[['comment_id','prediction']]\n",
    "\n",
    "df_test['score'] = df_test['prediction']\n",
    "df_test = df_test[['comment_id','score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.to_csv('./submission.csv', index=False)\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
