{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## 0.84+ score by ensemble of simple TF-Idf and Ridge regression\n\n### Ensemble of TfIdf - Ridge models using data from \n- Toxic competition\n- Toxic CLEANED competition\n- Ruddit toxic data\n- Toxic multilingual competition\n\n### Analysis of bad predictions\n","metadata":{}},{"cell_type":"markdown","source":"#### Some cool starters notebooks : \n- https://www.kaggle.com/julian3833/jigsaw-incredibly-simple-naive-bayes-0-768\n- https://www.kaggle.com/steubk/jrsotc-ridgeregression-ensemble-of-3","metadata":{}},{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.linear_model import Ridge, LinearRegression\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.base import TransformerMixin, BaseEstimator\n\nimport re \nimport scipy\nfrom scipy import sparse\nimport gc \n\nfrom IPython.display import display\nfrom pprint import pprint\nfrom matplotlib import pyplot as plt \n\nimport time\nimport scipy.optimize as optimize\nimport warnings\nwarnings.filterwarnings(\"ignore\")\npd.options.display.max_colwidth=300\npd.options.display.max_columns = 100\n","metadata":{"execution":{"iopub.status.busy":"2021-11-23T05:04:39.571666Z","iopub.execute_input":"2021-11-23T05:04:39.571942Z","iopub.status.idle":"2021-11-23T05:04:39.579887Z","shell.execute_reply.started":"2021-11-23T05:04:39.571914Z","shell.execute_reply":"2021-11-23T05:04:39.578673Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def timer(func):\n    def wrapper(*args, **kws):\n        st = time.time()\n        res = func(*args, **kws)\n        et = time.time()\n        tt = (et-st)/60\n        print(f'Time taken is {tt:.2f} mins')\n        return res\n    return wrapper\n","metadata":{"execution":{"iopub.status.busy":"2021-11-23T05:04:39.934387Z","iopub.execute_input":"2021-11-23T05:04:39.934903Z","iopub.status.idle":"2021-11-23T05:04:39.94037Z","shell.execute_reply.started":"2021-11-23T05:04:39.934858Z","shell.execute_reply":"2021-11-23T05:04:39.939477Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training data \n\n## Convert the label to SUM of all toxic labels (This might help with maintaining toxicity order of comments)","metadata":{}},{"cell_type":"code","source":"df_test = pd.read_csv(\"../input/jigsaw-toxic-comment-classification-challenge/test.csv\")\ndf_test_l = pd.read_csv(\"../input/jigsaw-toxic-comment-classification-challenge/test_labels.csv\").replace(-1,0)\nprint(df_test.shape)\ndf_test = pd.merge(df_test, df_test_l, how=\"left\", on = \"id\")\ndf_test.shape","metadata":{"execution":{"iopub.status.busy":"2021-11-23T05:06:29.916956Z","iopub.execute_input":"2021-11-23T05:06:29.917413Z","iopub.status.idle":"2021-11-23T05:06:32.016124Z","shell.execute_reply.started":"2021-11-23T05:06:29.917365Z","shell.execute_reply":"2021-11-23T05:06:32.015184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"../input/jigsaw-toxic-comment-classification-challenge/train.csv\")\nprint(df.shape)\ndf = pd.concat([df, df_test])\nprint(df.shape)\ndel df_test\n\nfor col in ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']:\n    print(f'****** {col} *******')\n    display(df.loc[df[col]==1,['comment_text',col]].sample(5))","metadata":{"execution":{"iopub.status.busy":"2021-11-23T05:06:32.017576Z","iopub.execute_input":"2021-11-23T05:06:32.017903Z","iopub.status.idle":"2021-11-23T05:06:33.160259Z","shell.execute_reply.started":"2021-11-23T05:06:32.017871Z","shell.execute_reply":"2021-11-23T05:06:33.159321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Give more weight to severe toxic \ndf['severe_toxic'] = df.severe_toxic * 2\ndf['y'] = (df[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].sum(axis=1) ).astype(int)\ndf['y'] = df['y']/df['y'].max()\n\ndf = df[['comment_text', 'y']].rename(columns={'comment_text': 'text'})\ndf.sample(5)","metadata":{"execution":{"iopub.status.busy":"2021-11-23T05:06:33.161976Z","iopub.execute_input":"2021-11-23T05:06:33.162301Z","iopub.status.idle":"2021-11-23T05:06:33.231068Z","shell.execute_reply.started":"2021-11-23T05:06:33.16226Z","shell.execute_reply":"2021-11-23T05:06:33.230084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['y'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-11-23T05:06:35.69122Z","iopub.execute_input":"2021-11-23T05:06:35.691574Z","iopub.status.idle":"2021-11-23T05:06:35.704322Z","shell.execute_reply.started":"2021-11-23T05:06:35.691541Z","shell.execute_reply":"2021-11-23T05:06:35.70303Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load validation data & filter for overlapping sentences","metadata":{}},{"cell_type":"code","source":"# Validation data \n\ndf_val = pd.read_csv(\"../input/jigsaw-toxic-severity-rating/validation_data.csv\")\nprint(df_val.shape)\n\n\n# Find cases already present in toxic data\n\ndf_val = pd.merge(df_val, df.loc[:,['text']], \n                  left_on = 'less_toxic', \n                  right_on = 'text', how='left')\n\ndf_val = pd.merge(df_val, df.loc[:,['text']], \n                  left_on = 'more_toxic', \n                  right_on = 'text', how='left')\n\n# Removing those cases\ndf_val = df_val[(~df_val.text_x.isna()) | (~df_val.text_y.isna())][['worker', 'less_toxic', 'more_toxic']]\ndf_val.shape","metadata":{"execution":{"iopub.status.busy":"2021-11-23T05:04:42.65083Z","iopub.execute_input":"2021-11-23T05:04:42.651229Z","iopub.status.idle":"2021-11-23T05:04:43.13288Z","shell.execute_reply.started":"2021-11-23T05:04:42.651181Z","shell.execute_reply":"2021-11-23T05:04:43.131983Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create 3 versions of the TOXIC data","metadata":{}},{"cell_type":"code","source":"n_folds = 2\n\nfrac_1 = 0.7\nfrac_1_factor = 1.3\n","metadata":{"execution":{"iopub.status.busy":"2021-11-20T07:56:23.334164Z","iopub.execute_input":"2021-11-20T07:56:23.334794Z","iopub.status.idle":"2021-11-20T07:56:23.33955Z","shell.execute_reply.started":"2021-11-20T07:56:23.334743Z","shell.execute_reply":"2021-11-20T07:56:23.338711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@timer\ndef create_folds():\n    for fld in range(n_folds):\n        print(f'Fold: {fld}')\n        tmp_df = pd.concat([df[df.y>0].sample(frac=frac_1, random_state = 10*(fld+1)) , \n                            df[df.y==0].sample(n=int(len(df[df.y>0])*frac_1*frac_1_factor) , \n                                                random_state = 10*(fld+1))], axis=0).sample(frac=1, random_state = 10*(fld+1))\n\n        tmp_df.to_csv(f'/kaggle/working/df_fld{fld}.csv', index=False)\n        print(tmp_df.shape)\n        print(tmp_df['y'].value_counts())\n\n\ncreate_folds()","metadata":{"execution":{"iopub.status.busy":"2021-11-20T02:11:40.670885Z","iopub.execute_input":"2021-11-20T02:11:40.671183Z","iopub.status.idle":"2021-11-20T02:11:42.280862Z","shell.execute_reply.started":"2021-11-20T02:11:40.671152Z","shell.execute_reply":"2021-11-20T02:11:42.279989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create 3 versions of __clean__ TOXIC data","metadata":{}},{"cell_type":"code","source":"@timer\ndef clean(data, col):\n\n    # Clean some punctutations\n    data[col] = data[col].str.replace('\\n', ' \\n ')\n    # Remove ip address\n    data[col] = data[col].str.replace(r'(([0-9]+\\.){2,}[0-9]+)',' ')\n    \n    data[col] = data[col].str.replace(r'([a-zA-Z]+)([/!?.])([a-zA-Z]+)',r'\\1 \\2 \\3')\n    # Replace repeating characters more than 3 times to length of 3\n    data[col] = data[col].str.replace(r'([*!?\\'])\\1\\1{2,}',r'\\1\\1\\1')\n    # patterns with repeating characters \n    data[col] = data[col].str.replace(r'([a-zA-Z])\\1{2,}\\b',r'\\1\\1')\n    data[col] = data[col].str.replace(r'([a-zA-Z])\\1\\1{2,}\\B',r'\\1\\1\\1')\n    data[col] = data[col].str.replace(r'[ ]{2,}',' ').str.strip()   \n    # Add space around repeating characters\n    data[col] = data[col].str.replace(r'([*!?\\']+)',r' \\1 ')    \n    \n    return data","metadata":{"execution":{"iopub.status.busy":"2021-11-23T05:16:56.365452Z","iopub.execute_input":"2021-11-23T05:16:56.367755Z","iopub.status.idle":"2021-11-23T05:16:56.374722Z","shell.execute_reply.started":"2021-11-23T05:16:56.367711Z","shell.execute_reply":"2021-11-23T05:16:56.373637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Test clean function\ntest_clean_df = pd.DataFrame({\"text\":\n                              [\"heyy\\n\\nkkdsfj\",\n                               \"hi   how/are/you ???\",\n                               \"hey?????\",\n                               \"hey????? 18.98.333.20 18.98.\",\n                               \"noooo!!!!!!!!!   comeone !! \",\n                              \"cooooooooool     brooooooooooo  coool brooo\",\n                              \"naaaahhhhhhh\"]})\ndisplay(test_clean_df)\nclean(test_clean_df,'text')","metadata":{"execution":{"iopub.status.busy":"2021-11-23T05:16:56.776209Z","iopub.execute_input":"2021-11-23T05:16:56.777185Z","iopub.status.idle":"2021-11-23T05:16:56.796622Z","shell.execute_reply.started":"2021-11-23T05:16:56.77714Z","shell.execute_reply":"2021-11-23T05:16:56.795512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = clean(df,'text')","metadata":{"execution":{"iopub.status.busy":"2021-11-20T02:12:26.206676Z","iopub.execute_input":"2021-11-20T02:12:26.207158Z","iopub.status.idle":"2021-11-20T02:13:17.413812Z","shell.execute_reply.started":"2021-11-20T02:12:26.207118Z","shell.execute_reply":"2021-11-20T02:13:17.412814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nfor fld in range(n_folds):\n    print(f'Fold: {fld}')\n    tmp_df = pd.concat([df[df.y>0].sample(frac=frac_1, random_state = 10*(fld+1)) , \n                        df[df.y==0].sample(n=int(len(df[df.y>0])*frac_1*frac_1_factor) , \n                                            random_state = 10*(fld+1))], axis=0).sample(frac=1, random_state = 10*(fld+1))\n\n    tmp_df.to_csv(f'/kaggle/working/df_clean_fld{fld}.csv', index=False)\n    print(tmp_df.shape)\n    print(tmp_df['y'].value_counts())","metadata":{"execution":{"iopub.status.busy":"2021-11-19T14:20:37.54783Z","iopub.execute_input":"2021-11-19T14:20:37.548167Z","iopub.status.idle":"2021-11-19T14:20:37.635183Z","shell.execute_reply.started":"2021-11-19T14:20:37.548124Z","shell.execute_reply":"2021-11-19T14:20:37.634359Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del df,tmp_df\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-11-19T14:20:37.953341Z","iopub.execute_input":"2021-11-19T14:20:37.954089Z","iopub.status.idle":"2021-11-19T14:20:38.07525Z","shell.execute_reply.started":"2021-11-19T14:20:37.954052Z","shell.execute_reply":"2021-11-19T14:20:38.074465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Read toxic Ruddit data","metadata":{}},{"cell_type":"code","source":"df_ = pd.read_csv(\"../input/ruddit-jigsaw-dataset/Dataset/ruddit_with_text.csv\")\nprint(df_.shape)\n\ndf_ = df_[['txt', 'offensiveness_score']].rename(columns={'txt': 'text',\n                                                                'offensiveness_score':'y'})\n\ndf_['y'] = (df_['y'] - df_.y.min()) / (df_.y.max() - df_.y.min()) \ndf_.y.hist()","metadata":{"execution":{"iopub.status.busy":"2021-11-19T14:20:39.511281Z","iopub.execute_input":"2021-11-19T14:20:39.511621Z","iopub.status.idle":"2021-11-19T14:20:39.855518Z","shell.execute_reply.started":"2021-11-19T14:20:39.511584Z","shell.execute_reply":"2021-11-19T14:20:39.854505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create 3 versions of RUDDIT data","metadata":{}},{"cell_type":"code","source":"\nfor fld in range(n_folds):\n    print(f'Fold: {fld}')\n    tmp_df = df_.sample(frac=frac_1, random_state = 10*(fld+1))\n    tmp_df.to_csv(f'/kaggle/working/df2_fld{fld}.csv', index=False)\n    print(tmp_df.shape)\n    print(tmp_df['y'].value_counts())","metadata":{"execution":{"iopub.status.busy":"2021-11-19T14:20:42.060503Z","iopub.execute_input":"2021-11-19T14:20:42.060895Z","iopub.status.idle":"2021-11-19T14:20:42.255707Z","shell.execute_reply.started":"2021-11-19T14:20:42.060852Z","shell.execute_reply":"2021-11-19T14:20:42.254753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del tmp_df, df_; \ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-11-19T14:20:42.280008Z","iopub.execute_input":"2021-11-19T14:20:42.280811Z","iopub.status.idle":"2021-11-19T14:20:42.454622Z","shell.execute_reply.started":"2021-11-19T14:20:42.280758Z","shell.execute_reply":"2021-11-19T14:20:42.453756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Read Jigsaw multilingual data CLEANED","metadata":{}},{"cell_type":"code","source":"dfm = pd.read_csv(\"../input/jigsaw-multilingual-toxic-comment-classification/jigsaw-toxic-comment-train.csv\")\nprint(dfm.shape)\n\ndfm = clean(dfm,'comment_text')\n\nfor col in ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']:\n    print(f'****** {col} *******')\n    display(dfm.loc[dfm[col]==1,['comment_text',col]].sample(5))\n    \n\n# Give more weight to severe toxic \ndfm['severe_toxic'] = dfm.severe_toxic * 2\ndfm['y'] = (dfm[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].sum(axis=1) ).astype(int)\ndfm['y'] = dfm['y']/dfm['y'].max()\n\ndfm = dfm[['comment_text', 'y']].rename(columns={'comment_text': 'text'})\ndfm.y.value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-11-23T05:10:39.119207Z","iopub.execute_input":"2021-11-23T05:10:39.119589Z","iopub.status.idle":"2021-11-23T05:10:42.001364Z","shell.execute_reply.started":"2021-11-23T05:10:39.119554Z","shell.execute_reply":"2021-11-23T05:10:42.000338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create 3 versions of Multilingual data","metadata":{}},{"cell_type":"code","source":"\nfor fld in range(n_folds):\n    print(f'Fold: {fld}')\n    tmp_df = pd.concat([dfm[dfm.y>0].sample(frac=frac_1, random_state = 10*(fld+1)) , \n                        dfm[dfm.y==0].sample(n=int(len(dfm[dfm.y>0])*frac_1*frac_1_factor) , \n                                            random_state = 10*(fld+1))], axis=0).sample(frac=1, \n                                                                                        random_state = 10*(fld+1))\n\n    tmp_df.to_csv(f'/kaggle/working/dfm_fld{fld}.csv', index=False)\n    print(tmp_df.shape)\n    print(tmp_df['y'].value_counts())","metadata":{"execution":{"iopub.status.busy":"2021-11-20T07:56:36.147976Z","iopub.execute_input":"2021-11-20T07:56:36.148262Z","iopub.status.idle":"2021-11-20T07:56:37.351457Z","shell.execute_reply.started":"2021-11-20T07:56:36.148234Z","shell.execute_reply":"2021-11-20T07:56:37.350476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load Test data  \n","metadata":{}},{"cell_type":"code","source":"# Validation data \n\n# df_val = pd.read_csv(\"../input/jigsaw-toxic-severity-rating/validation_data.csv\")\n# df_val.shape","metadata":{"execution":{"iopub.status.busy":"2021-11-20T07:56:45.678906Z","iopub.execute_input":"2021-11-20T07:56:45.679158Z","iopub.status.idle":"2021-11-20T07:56:46.238798Z","shell.execute_reply.started":"2021-11-20T07:56:45.679132Z","shell.execute_reply":"2021-11-20T07:56:46.237903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Remove contradicting cases from validation data\n- cases where contradictory evaluation is in minority (< 50%)","metadata":{}},{"cell_type":"code","source":"# gp1=df_val.copy()\n# gp1['pair'] = gp1.apply(lambda x:\" \".join(sorted((x['less_toxic'],\n#                                                   x['more_toxic']))),axis=1)\n# gp1['pair_hash'] = gp1.pair.apply(lambda x: str(abs(hash(x)) % (10 ** 8)))\n# del gp1['pair']\n# print(len(gp1), len(gp1.pair_hash.drop_duplicates()))\n\n# gp1['cnt']=gp1.groupby(['pair_hash', \n#                         'less_toxic',\n#                         'more_toxic']).transform(lambda x: x.count())\n# print(gp1[['pair_hash', 'less_toxic', 'more_toxic','cnt']].drop_duplicates().cnt.value_counts())\n\n# #gp1.head(10)\n# majority_cases = gp1.groupby('pair_hash')\\\n#                     .agg({'cnt':['count','max']})\\\n#                     .reset_index()\\\n#                     .set_axis(['pair_hash','count','max'], \n#                               axis='columns')\\\n#                     .assign(pct=lambda x: x['max']/x['count'])\\\n#                     .query('pct>=0.5')\\\n#                     .rename(columns={'max':'cnt'})\\\n#                     [['pair_hash','cnt']]\n\n# df_val = pd.merge(gp1,majority_cases,\n#                  how=\"inner\",\n#                  on = ['pair_hash','cnt'])\n# #gp1.groupby('pair_hash').apply(lambda x: x[['less_toxic','more_toxic','cnt']].sort_values('cnt', ascending=False))\n# df_val.shape","metadata":{"execution":{"iopub.status.busy":"2021-11-20T02:29:24.674687Z","iopub.execute_input":"2021-11-20T02:29:24.675269Z","iopub.status.idle":"2021-11-20T02:29:47.320157Z","shell.execute_reply.started":"2021-11-20T02:29:24.675232Z","shell.execute_reply":"2021-11-20T02:29:47.31934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Test data\n\ndf_sub = pd.read_csv(\"../input/jigsaw-toxic-severity-rating/comments_to_score.csv\")\ndf_sub.shape","metadata":{"execution":{"iopub.status.busy":"2021-11-20T07:56:48.86945Z","iopub.execute_input":"2021-11-20T07:56:48.86974Z","iopub.status.idle":"2021-11-20T07:56:48.989723Z","shell.execute_reply.started":"2021-11-20T07:56:48.86971Z","shell.execute_reply":"2021-11-20T07:56:48.988735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create Sklearn Pipeline with \n-  TFIDF - Take 'char_wb' as analyzer to capture subwords well\n-  Ridge - Ridge is a simple regression algorithm that will reduce overfitting ","metadata":{}},{"cell_type":"code","source":"\nclass LengthTransformer(BaseEstimator, TransformerMixin):\n\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        return sparse.csr_matrix([[(len(x)-360)/550] for x in X])\n    def get_feature_names(self):\n        return [\"lngth\"]\n\nclass LengthUpperTransformer(BaseEstimator, TransformerMixin):\n\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        return sparse.csr_matrix([[int(sum([1 for y in x if y.isupper()])/len(x) > 0.75) ] for x in X])\n    def get_feature_names(self):\n        return [\"lngth_uppercase\"]","metadata":{"execution":{"iopub.status.busy":"2021-11-20T07:56:05.964461Z","iopub.execute_input":"2021-11-20T07:56:05.965008Z","iopub.status.idle":"2021-11-20T07:56:05.971977Z","shell.execute_reply.started":"2021-11-20T07:56:05.964971Z","shell.execute_reply":"2021-11-20T07:56:05.97097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Does % of uppercase characters have effect on toxicity\n","metadata":{}},{"cell_type":"code","source":"\n# df_val['upper_1'] = np.array(LengthUpperTransformer().transform(df_val['less_toxic']).todense()).reshape(-1,1)\n# df_val['upper_2'] = np.array(LengthUpperTransformer().transform(df_val['more_toxic']).todense()).reshape(-1,1)\n\n# print(df_val['upper_1'].mean(), df_val['upper_1'].std())\n# print(df_val['upper_2'].mean(), df_val['upper_2'].std())\n\n# df_val['upper_1'].hist(bins=100)\n# df_val['upper_2'].hist(bins=100)","metadata":{"execution":{"iopub.status.busy":"2021-11-18T17:00:59.086833Z","iopub.execute_input":"2021-11-18T17:00:59.087117Z","iopub.status.idle":"2021-11-18T17:01:01.13661Z","shell.execute_reply.started":"2021-11-18T17:00:59.087089Z","shell.execute_reply":"2021-11-18T17:01:01.13564Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train pipeline\n\n- Load folds data\n- train pipeline\n- Predict on validation data\n- Predict on test data","metadata":{}},{"cell_type":"markdown","source":"# Training function","metadata":{}},{"cell_type":"code","source":"@timer\ndef train_pipeline(pipeline, data_path_name, n_folds, clean_prm = False):\n    val_preds_arr1_tmp = np.zeros((df_val.shape[0], n_folds))\n    val_preds_arr2_tmp = np.zeros((df_val.shape[0], n_folds))\n    test_preds_arr_tmp = np.zeros((df_sub.shape[0], n_folds))\n\n    for fld in range(n_folds):\n        print(\"\\n\\n\")\n        print(f' ****************************** FOLD: {fld} ******************************')\n        df = pd.read_csv(f'/kaggle/working/{data_path_name}_fld{fld}.csv')\n        print(df.shape)\n\n        print(\"\\nTrain:\")\n        # Train the pipeline\n        pipeline.fit(df['text'], df['y'])\n\n        # What are the important features for toxicity\n\n        print('\\nTotal number of features:', len(pipeline['features'].get_feature_names()) )\n\n        feature_wts = sorted(list(zip(pipeline['features'].get_feature_names(), \n                                      np.round(pipeline['clf'].coef_,2) )), \n                             key = lambda x:x[1], \n                             reverse=True)\n\n        display(pd.DataFrame(feature_wts[:50], columns = ['feat','val']).T)\n        #.plot('feat','val',kind='barh',figsize = (8,8) )\n        #plt.show()\n\n        if clean_prm:\n            print(\"\\npredict validation data \")\n            val_preds_arr1_tmp[:,fld] = pipeline.predict(clean(df_val,'less_toxic')['less_toxic'])\n            val_preds_arr2_tmp[:,fld] = pipeline.predict(clean(df_val,'more_toxic')['more_toxic'])\n\n            print(\"\\npredict test data \")\n            test_preds_arr_tmp[:,fld] = pipeline.predict(clean(df_sub,'text')['text'])\n        else:\n            print(\"\\npredict validation data \")\n            val_preds_arr1_tmp[:,fld] = pipeline.predict(df_val['less_toxic'])\n            val_preds_arr2_tmp[:,fld] = pipeline.predict(df_val['more_toxic'])\n\n            print(\"\\npredict test data \")\n            test_preds_arr_tmp[:,fld] = pipeline.predict(df_sub['text'])\n    return val_preds_arr1_tmp, val_preds_arr2_tmp, test_preds_arr_tmp","metadata":{"execution":{"iopub.status.busy":"2021-11-20T08:15:11.741625Z","iopub.execute_input":"2021-11-20T08:15:11.741933Z","iopub.status.idle":"2021-11-20T08:15:11.750858Z","shell.execute_reply.started":"2021-11-20T08:15:11.741901Z","shell.execute_reply":"2021-11-20T08:15:11.750115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Toxic Training","metadata":{}},{"cell_type":"code","source":"features = FeatureUnion([\n    #('vect1', LengthTransformer()),\n    #('vect2', LengthUpperTransformer()),\n    (\"vect3\", TfidfVectorizer(min_df= 3, max_df=0.5, \n                              analyzer = 'char_wb', ngram_range = (3,5))),\n    #(\"vect4\", TfidfVectorizer(min_df= 5, max_df=0.5, analyzer = 'word', token_pattern=r'(?u)\\b\\w{8,}\\b')),\n\n])\npipeline = Pipeline(\n    [\n        (\"features\", features),\n        #(\"clf\", RandomForestRegressor(n_estimators = 5, min_sample_leaf=3)),\n        (\"clf\", Ridge()),\n        #(\"clf\",LinearRegression())\n    ]\n)\n\nval_preds_arr1, val_preds_arr2, test_preds_arr = train_pipeline(pipeline, \n                                                                \"df\", \n                                                                n_folds,\n                                                                clean_prm=False)\n","metadata":{"execution":{"iopub.status.busy":"2021-11-19T14:26:40.71077Z","iopub.execute_input":"2021-11-19T14:26:40.711385Z","iopub.status.idle":"2021-11-19T14:29:08.144162Z","shell.execute_reply.started":"2021-11-19T14:26:40.711343Z","shell.execute_reply":"2021-11-19T14:29:08.143211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Toxic __clean__ Training","metadata":{}},{"cell_type":"code","source":"features = FeatureUnion([\n    #('vect2', LengthUpperTransformer()),\n    (\"vect3\", TfidfVectorizer(min_df= 3, max_df=0.5, \n                              analyzer = 'char_wb', ngram_range = (3,5))),\n\n])\npipeline = Pipeline(\n    [\n        (\"features\", features),\n        #(\"clf\", RandomForestRegressor(n_estimators = 5, min_sample_leaf=3)),\n        (\"clf\", Ridge()),\n    ]\n)\n\nval_preds_arr1c, val_preds_arr2c, test_preds_arrc = train_pipeline(pipeline, \n                                                                   \"df_clean\", \n                                                                   n_folds,\n                                                                   clean_prm=True)\n","metadata":{"execution":{"iopub.status.busy":"2021-11-19T14:29:08.146351Z","iopub.execute_input":"2021-11-19T14:29:08.146697Z","iopub.status.idle":"2021-11-19T14:31:34.302377Z","shell.execute_reply.started":"2021-11-19T14:29:08.14665Z","shell.execute_reply":"2021-11-19T14:31:34.301141Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Ruddit data Training","metadata":{}},{"cell_type":"code","source":"features = FeatureUnion([\n    #('vect2', LengthUpperTransformer()),\n    (\"vect3\", TfidfVectorizer(min_df= 3, max_df=0.5, \n                              analyzer = 'char_wb', ngram_range = (3,5))),\n\n])\npipeline = Pipeline(\n    [\n        (\"features\", features),\n        #(\"clf\", RandomForestRegressor(n_estimators = 5, min_sample_leaf=3)),\n        (\"clf\", Ridge()),\n    ]\n)\n\nval_preds_arr1_, val_preds_arr2_, test_preds_arr_ = train_pipeline(pipeline, \n                                                                   \"df2\", \n                                                                   n_folds,\n                                                                   clean_prm=False)\n","metadata":{"execution":{"iopub.status.busy":"2021-11-19T14:31:34.304127Z","iopub.execute_input":"2021-11-19T14:31:34.30438Z","iopub.status.idle":"2021-11-19T14:34:06.98341Z","shell.execute_reply.started":"2021-11-19T14:31:34.304346Z","shell.execute_reply":"2021-11-19T14:34:06.982735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Mulitlingual data Training","metadata":{}},{"cell_type":"code","source":"features = FeatureUnion([\n    #('vect1', LengthTransformer()),\n    #('vect2', LengthUpperTransformer()),\n    (\"vect3\", TfidfVectorizer(min_df= 3, max_df=0.5, \n                              analyzer = 'char_wb', ngram_range = (3,5))),\n    #(\"vect4\", CountVectorizer(min_df= 5, max_df=0.3, analyzer = 'word', ngram_range = (2,3), token_pattern=r'(?u)\\b\\w{3,}\\b', binary=True))\n])\npipeline = Pipeline(\n    [\n        (\"features\", features),\n        #(\"clf\", RandomForestRegressor(n_estimators = 5, min_sample_leaf=3)),\n        (\"clf\", Ridge()),\n        #(\"clf\",LinearRegression())\n    ]\n)\n\nval_preds_arr1m, val_preds_arr2m, test_preds_arrm = train_pipeline(pipeline, \n                                                                    \"dfm\", \n                                                                    n_folds,\n                                                                   clean_prm=True)\n","metadata":{"execution":{"iopub.status.busy":"2021-11-20T08:15:55.825178Z","iopub.execute_input":"2021-11-20T08:15:55.82547Z","iopub.status.idle":"2021-11-20T08:18:15.714669Z","shell.execute_reply.started":"2021-11-20T08:15:55.825439Z","shell.execute_reply":"2021-11-20T08:18:15.713014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# del df, pipeline, feature_wts\n# gc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-11-19T14:34:06.985534Z","iopub.execute_input":"2021-11-19T14:34:06.98588Z","iopub.status.idle":"2021-11-19T14:34:07.006505Z","shell.execute_reply.started":"2021-11-19T14:34:06.985812Z","shell.execute_reply":"2021-11-19T14:34:07.004579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Validate the pipeline ","metadata":{}},{"cell_type":"code","source":"print(\" Toxic data \")\np1 = val_preds_arr1.mean(axis=1)\np2 = val_preds_arr2.mean(axis=1)\n\nprint(f'Validation Accuracy is { np.round((p1 < p2).mean() * 100,2)}')\n\nprint(\" Ruddit data \")\np3 = val_preds_arr1_.mean(axis=1)\np4 = val_preds_arr2_.mean(axis=1)\n\nprint(f'Validation Accuracy is { np.round((p3 < p4).mean() * 100,2)}')\n\nprint(\" Toxic CLEAN data \")\np5 = val_preds_arr1c.mean(axis=1)\np6 = val_preds_arr2c.mean(axis=1)\n\nprint(f'Validation Accuracy is { np.round((p5 < p6).mean() * 100,2)}')\n\nprint(\" Toxic Mulitlingual data \")\np7 = val_preds_arr1m.mean(axis=1)\np8 = val_preds_arr2m.mean(axis=1)\n\nprint(f'Validation Accuracy is { np.round((p7 < p8).mean() * 100,2)}')\n","metadata":{"execution":{"iopub.status.busy":"2021-11-19T16:25:35.16649Z","iopub.execute_input":"2021-11-19T16:25:35.166756Z","iopub.status.idle":"2021-11-19T16:25:35.174286Z","shell.execute_reply.started":"2021-11-19T16:25:35.166728Z","shell.execute_reply":"2021-11-19T16:25:35.173426Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Optimize the model weights for ensemble","metadata":{}},{"cell_type":"code","source":"\n@timer\ndef optimize_wts():\n    func = lambda x: -1*(((x[0]*p1 + x[1]*p3 + x[2]*p5 + x[3]*p7) < (x[0]*p2 + x[1]*p4 + x[2]*p6  + x[3]*p8)).mean())\n\n    rranges = (slice(0.20, 0.6, 0.015), \n               slice(0.05, 0.5, 0.015),\n               slice(0.05, 0.5, 0.015),\n               slice(0.05, 0.5, 0.015),\n              )\n\n    resbrute = optimize.brute(func, \n                              rranges, \n                              #args=params, \n                              full_output=True,\n                              finish=None)\n    return resbrute\nresbrute = optimize_wts()\n\nprint(resbrute[0])  # global minimum\nprint(resbrute[1]*-1)  # function value at global minimum\n    ","metadata":{"execution":{"iopub.status.busy":"2021-11-19T15:04:18.596778Z","iopub.execute_input":"2021-11-19T15:04:18.597445Z","iopub.status.idle":"2021-11-19T15:04:24.027032Z","shell.execute_reply.started":"2021-11-19T15:04:18.597395Z","shell.execute_reply":"2021-11-19T15:04:24.026065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"w1,w2,w3,w4 = resbrute[0]\n#print(best_wts)\n\np1_wt = w1*p1 + w2*p3 + w3*p5 + w4*p7\np2_wt = w1*p2 + w2*p4 + w3*p6 + w4*p8\n","metadata":{"execution":{"iopub.status.busy":"2021-11-19T15:04:41.634469Z","iopub.execute_input":"2021-11-19T15:04:41.634955Z","iopub.status.idle":"2021-11-19T15:04:41.640759Z","shell.execute_reply.started":"2021-11-19T15:04:41.634919Z","shell.execute_reply":"2021-11-19T15:04:41.640122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Analyze bad predictions \n### Incorrect predictions with similar scores\n### Incorrect predictions with different scores","metadata":{}},{"cell_type":"code","source":"df_val['p1'] = p1_wt\ndf_val['p2'] = p2_wt\ndf_val['diff'] = np.abs(p2_wt - p1_wt)\n\ndf_val['correct'] = (p1_wt < p2_wt).astype('int')\n","metadata":{"execution":{"iopub.status.busy":"2021-11-19T15:04:44.008901Z","iopub.execute_input":"2021-11-19T15:04:44.009203Z","iopub.status.idle":"2021-11-19T15:04:44.017792Z","shell.execute_reply.started":"2021-11-19T15:04:44.009173Z","shell.execute_reply":"2021-11-19T15:04:44.017102Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n### Incorrect predictions with similar scores\n\ndf_val[(df_val.correct == 0) & (df_val.p1 < 0.5*df_val.p1.max())].sort_values('diff', ascending=True).head(20)","metadata":{"execution":{"iopub.status.busy":"2021-11-18T17:25:58.138899Z","iopub.execute_input":"2021-11-18T17:25:58.139517Z","iopub.status.idle":"2021-11-18T17:25:58.165802Z","shell.execute_reply.started":"2021-11-18T17:25:58.139465Z","shell.execute_reply":"2021-11-18T17:25:58.164756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_val[(df_val.correct == 0) & (df_val.p1 > 0.5*df_val.p1.max())].sort_values('diff', ascending=True).head(20)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Some of these just look incorrectly tagged \n","metadata":{}},{"cell_type":"code","source":"### Incorrect predictions with dis-similar scores\n\ndf_val[df_val.correct == 0].sort_values('diff', ascending=False).head(20)","metadata":{"execution":{"iopub.status.busy":"2021-11-18T17:26:02.870889Z","iopub.execute_input":"2021-11-18T17:26:02.871184Z","iopub.status.idle":"2021-11-18T17:26:02.892047Z","shell.execute_reply.started":"2021-11-18T17:26:02.871148Z","shell.execute_reply":"2021-11-18T17:26:02.891101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_val[(df_val.correct == 0) & (df_val['diff'] < 0.4*df_val['diff'].max())].sort_values('diff', ascending=False).head(20)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Predict on test data ","metadata":{}},{"cell_type":"code","source":"# Predict using pipeline\n\ndf_sub['score'] = w1*test_preds_arr.mean(axis=1) + \\\n                  w2*test_preds_arr_.mean(axis=1) + \\\n                  w3*test_preds_arrc.mean(axis=1) + \\\n                  w4*test_preds_arrm.mean(axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-11-18T17:27:08.378049Z","iopub.execute_input":"2021-11-18T17:27:08.37834Z","iopub.status.idle":"2021-11-18T17:27:08.38374Z","shell.execute_reply.started":"2021-11-18T17:27:08.37831Z","shell.execute_reply":"2021-11-18T17:27:08.383073Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#test_preds_arr","metadata":{"execution":{"iopub.status.busy":"2021-11-18T14:06:11.374494Z","iopub.execute_input":"2021-11-18T14:06:11.374853Z","iopub.status.idle":"2021-11-18T14:06:11.387837Z","shell.execute_reply.started":"2021-11-18T14:06:11.374806Z","shell.execute_reply":"2021-11-18T14:06:11.386874Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Correct the rank ordering","metadata":{}},{"cell_type":"code","source":"# Cases with duplicates scores\n\ndf_sub['score'].count() - df_sub['score'].nunique()","metadata":{"execution":{"iopub.status.busy":"2021-11-18T17:27:13.966573Z","iopub.execute_input":"2021-11-18T17:27:13.96751Z","iopub.status.idle":"2021-11-18T17:27:13.976928Z","shell.execute_reply.started":"2021-11-18T17:27:13.96745Z","shell.execute_reply":"2021-11-18T17:27:13.976294Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"same_score = df_sub['score'].value_counts().reset_index()[:10]\nsame_score","metadata":{"execution":{"iopub.status.busy":"2021-11-18T17:27:15.139345Z","iopub.execute_input":"2021-11-18T17:27:15.139613Z","iopub.status.idle":"2021-11-18T17:27:15.155436Z","shell.execute_reply.started":"2021-11-18T17:27:15.139584Z","shell.execute_reply":"2021-11-18T17:27:15.154455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_sub[df_sub['score'].isin(same_score['index'].tolist())]","metadata":{"execution":{"iopub.status.busy":"2021-11-18T17:27:25.256581Z","iopub.execute_input":"2021-11-18T17:27:25.257391Z","iopub.status.idle":"2021-11-18T17:27:25.272575Z","shell.execute_reply.started":"2021-11-18T17:27:25.257354Z","shell.execute_reply":"2021-11-18T17:27:25.271921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Same comments have same score - which is ok ","metadata":{"execution":{"iopub.status.busy":"2021-11-18T17:27:29.327817Z","iopub.execute_input":"2021-11-18T17:27:29.328366Z","iopub.status.idle":"2021-11-18T17:27:29.331633Z","shell.execute_reply.started":"2021-11-18T17:27:29.328327Z","shell.execute_reply":"2021-11-18T17:27:29.331016Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Rank the predictions \n\n# df_sub['score']  = scipy.stats.rankdata(df_sub['score'], method='ordinal')\n\n# print(df_sub['score'].rank().nunique())","metadata":{"execution":{"iopub.status.busy":"2021-11-18T14:06:11.47096Z","iopub.execute_input":"2021-11-18T14:06:11.471643Z","iopub.status.idle":"2021-11-18T14:06:11.482114Z","shell.execute_reply.started":"2021-11-18T14:06:11.471594Z","shell.execute_reply":"2021-11-18T14:06:11.481373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_sub[['comment_id', 'score']].to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2021-11-18T17:27:30.915528Z","iopub.execute_input":"2021-11-18T17:27:30.916055Z","iopub.status.idle":"2021-11-18T17:27:30.944486Z","shell.execute_reply.started":"2021-11-18T17:27:30.915994Z","shell.execute_reply":"2021-11-18T17:27:30.943441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}